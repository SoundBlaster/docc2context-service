# Task 5.3: Set Up Log Aggregation

**Status:** PLAN
**Date:** 2026-01-13
**Priority:** Critical (Blocking Production)
**Effort:** 4-6 hours
**Dependencies:** Task 3.3 (Documentation) ✅

---

## Executive Summary

Implement centralized log aggregation using the ELK Stack (Elasticsearch, Logstash, Kibana) to collect, analyze, and retain application logs. This provides:

- **Centralized logging** for extraction operations, security events, and performance monitoring
- **Log retention** with 90-day retention for security events, 30-day for operational logs
- **Searchable dashboards** for extraction failures, failed authentication, rate limits, and performance anomalies
- **Production readiness** for security audits and incident response

---

## Architecture

### Stack Selection: ELK (Elasticsearch, Logstash, Kibana)

**Why ELK?**
- Open-source and cost-effective
- Docker-compatible for existing infrastructure
- Proven for high-volume logging
- Rich query language (Lucene/KQL) for analysis
- Kibana for visual dashboards and alerting

**Components:**
1. **Elasticsearch** (port 9200) - Distributed search and analytics engine
2. **Logstash** (port 5000) - Log ingestion and transformation pipeline
3. **Kibana** (port 5601) - Visualization and dashboards
4. **Application** - Enhanced structured JSON logging

---

## Implementation Plan

### Phase 1: Docker & Infrastructure Setup

#### 1.1 Update `docker-compose.yml`

Add ELK Stack services:

```yaml
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false  # Development; enable for production
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - docc2context-network
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logstash_patterns:/usr/share/logstash/patterns
    ports:
      - "5000:5000"
    environment:
      - LS_JAVA_OPTS=-Xmx256m -Xms256m
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - docc2context-network

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - docc2context-network
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status"]
      interval: 10s
      timeout: 5s
      retries: 10

volumes:
  elasticsearch_data:
  alertmanager_data:
  prometheus_data:

networks:
  docc2context-network:
    driver: bridge
```

**Changes:**
- Add `elasticsearch_data` volume for persistent storage
- Configure single-node Elasticsearch cluster
- Set Java heap sizes for resource constraints
- Add health checks for service readiness
- Use existing `docc2context-network`

#### 1.2 Create `logstash.conf`

Logstash configuration for log ingestion:

```conf
input {
  stdin {
    codec => json
  }
}

filter {
  # Parse JSON logs from application
  if [type] == "application" {
    # Extract timestamp if ISO format
    if [timestamp] {
      date {
        match => [ "timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }

    # Add metadata tags for filtering
    if [level] == "ERROR" or [level] == "CRITICAL" {
      mutate {
        add_tag => [ "error", "alert_candidate" ]
      }
    }

    if [event_type] == "extraction_failure" {
      mutate {
        add_tag => [ "extraction", "failure" ]
      }
    }

    if [event_type] == "auth_failure" {
      mutate {
        add_tag => [ "security", "auth_failure" ]
      }
    }

    if [event_type] == "rate_limit" {
      mutate {
        add_tag => [ "security", "rate_limit" ]
      }
    }

    # GeoIP enrichment (optional for IP logging)
    if [client_ip] and [client_ip] != "-" {
      geoip {
        source => "client_ip"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "docc2context-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
}
```

**Features:**
- JSON codec for structured logging
- Automatic timestamp parsing (ISO8601)
- Tag-based filtering for different log types
- Optional GeoIP enrichment
- Time-based index rotation (daily indices)

#### 1.3 Create `logstash_patterns/` (Custom Patterns Directory)

Create pattern definitions for Logstash if needed:

```
patterns/
├── docc2context
└── extraction
```

---

### Phase 2: Application Logging Enhancement

#### 2.1 Update `app/core/logging.py`

Enhance structured JSON logging to include ELK-specific fields:

```python
import json
import logging
import sys
from typing import Any, Dict, Optional
from datetime import datetime
from pythonjsonlogger import jsonlogger

# Structured logging fields for ELK
def setup_logging(env: str = "development"):
    """Configure structured JSON logging for ELK Stack."""

    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG if env == "development" else logging.INFO)

    # Remove default handlers
    logger.handlers.clear()

    # JSON formatter
    formatter = jsonlogger.JsonFormatter(
        '%(timestamp)s %(level)s %(name)s %(message)s %(request_id)s'
    )

    # Stdout handler (Docker/container logs)
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    return logger

class StructuredLogger:
    """Wrapper for structured logging with ELK metadata."""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def log_extraction(
        self,
        status: str,  # "success" or "failure"
        file_name: str,
        file_size: int,
        extraction_time: float,
        error_msg: Optional[str] = None,
        request_id: Optional[str] = None
    ):
        """Log extraction event for dashboard aggregation."""
        payload = {
            "event_type": "extraction",
            "status": status,
            "file_name": file_name,
            "file_size_bytes": file_size,
            "extraction_time_seconds": extraction_time,
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
        }
        if error_msg:
            payload["error_message"] = error_msg

        level = logging.ERROR if status == "failure" else logging.INFO
        self.logger.log(level, json.dumps(payload))

    def log_auth_failure(
        self,
        username: Optional[str],
        ip_address: str,
        reason: str,
        request_id: Optional[str] = None
    ):
        """Log authentication failure for security dashboard."""
        payload = {
            "event_type": "auth_failure",
            "username": username,
            "client_ip": ip_address,
            "reason": reason,
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "level": "WARNING",
        }
        self.logger.warning(json.dumps(payload))

    def log_rate_limit(
        self,
        ip_address: str,
        endpoint: str,
        limit: int,
        window: str,
        request_id: Optional[str] = None
    ):
        """Log rate limit trigger for security dashboard."""
        payload = {
            "event_type": "rate_limit",
            "client_ip": ip_address,
            "endpoint": endpoint,
            "limit": limit,
            "window": window,
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "level": "WARNING",
        }
        self.logger.warning(json.dumps(payload))

    def log_performance_anomaly(
        self,
        metric_name: str,
        value: float,
        threshold: float,
        request_id: Optional[str] = None
    ):
        """Log performance anomalies."""
        payload = {
            "event_type": "performance_anomaly",
            "metric": metric_name,
            "value": value,
            "threshold": threshold,
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
        }
        self.logger.warning(json.dumps(payload))
```

**Changes:**
- Add `python-json-logger` to requirements.txt
- Structured event logging with specific fields
- Dedicated methods for extraction, auth, rate limit, and performance events
- ISO8601 timestamps for Logstash parsing

#### 2.2 Update `app/main.py`

Initialize structured logging and integrate with Logstash:

```python
from app.core.logging import setup_logging, StructuredLogger

# At startup
@app.on_event("startup")
async def startup_event():
    setup_logging(env=settings.environment)
    logger.info("DocC2Context Service started", extra={"service": "docc2context"})

# Create structured logger instance
structured_logger = StructuredLogger(__name__)

# Update conversion endpoint to log extraction events
@app.post("/api/v1/convert")
async def convert(file: UploadFile, request: Request):
    request_id = request.headers.get("X-Request-ID", str(uuid4()))

    try:
        # ... extraction logic ...
        extraction_time = time.time() - start_time
        structured_logger.log_extraction(
            status="success",
            file_name=file.filename,
            file_size=len(file_content),
            extraction_time=extraction_time,
            request_id=request_id
        )
    except Exception as e:
        structured_logger.log_extraction(
            status="failure",
            file_name=file.filename,
            file_size=0,
            extraction_time=time.time() - start_time,
            error_msg=str(e),
            request_id=request_id
        )
        raise
```

---

### Phase 3: Log Retention & Archival

#### 3.1 Create `logstash_retention.conf` (Index Lifecycle Policy)

Define retention policies using Elasticsearch Index Lifecycle Management (ILM):

```json
{
  "policy": "docc2context-retention",
  "phases": {
    "hot": {
      "min_age": "0d",
      "actions": {
        "rollover": {
          "max_primary_store_size": "50gb",
          "max_age": "1d"
        }
      }
    },
    "warm": {
      "min_age": "30d",
      "actions": {
        "set_priority": {
          "priority": 50
        }
      }
    },
    "cold": {
      "min_age": "90d",
      "actions": {
        "set_priority": {
          "priority": 0
        }
      }
    },
    "delete": {
      "min_age": "365d",
      "actions": {
        "delete": {}
      }
    }
  }
}
```

**Retention Strategy:**
- Hot (0-30d): Real-time indexing, full search
- Warm (30-90d): Read-only, optimized storage
- Cold (90d-365d): Archived, slow storage
- Delete (365d+): Permanent removal

#### 3.2 Create `scripts/setup_elasticsearch.sh`

Initialize Elasticsearch indices and policies:

```bash
#!/bin/bash

# Wait for Elasticsearch to be ready
until curl -s http://elasticsearch:9200/_cluster/health; do
  sleep 2
done

# Create ILM policy for log retention
curl -X PUT "http://elasticsearch:9200/_ilm/policy/docc2context-retention" \
  -H "Content-Type: application/json" \
  -d @logstash_retention.conf

# Create index template with retention policy
curl -X PUT "http://elasticsearch:9200/_index_template/docc2context" \
  -H "Content-Type: application/json" \
  -d '{
    "index_patterns": ["docc2context-*"],
    "template": {
      "settings": {
        "index.lifecycle.name": "docc2context-retention",
        "index.lifecycle.rollover_alias": "docc2context"
      },
      "mappings": {
        "properties": {
          "event_type": { "type": "keyword" },
          "status": { "type": "keyword" },
          "file_name": { "type": "keyword" },
          "file_size_bytes": { "type": "long" },
          "extraction_time_seconds": { "type": "float" },
          "client_ip": { "type": "ip" },
          "endpoint": { "type": "keyword" },
          "request_id": { "type": "keyword" },
          "timestamp": { "type": "date" }
        }
      }
    }
  }'

echo "Elasticsearch configured successfully"
```

---

### Phase 4: Kibana Dashboards

#### 4.1 Create `dashboards/extraction_failures.json`

Kibana dashboard for monitoring extraction failures:

```json
{
  "dashboard": {
    "title": "Extraction Failures",
    "panels": [
      {
        "title": "Failure Rate Over Time",
        "visualization": {
          "type": "area",
          "query": "event_type:extraction AND status:failure",
          "xAxisField": "@timestamp",
          "yAxisField": "count"
        }
      },
      {
        "title": "Top 10 Files by Failure",
        "visualization": {
          "type": "table",
          "query": "event_type:extraction AND status:failure",
          "metrics": [
            {"field": "file_name", "type": "terms"},
            {"field": "@timestamp", "type": "count"}
          ]
        }
      },
      {
        "title": "Failure Reasons",
        "visualization": {
          "type": "pie",
          "query": "event_type:extraction AND status:failure",
          "groupBy": "error_message"
        }
      },
      {
        "title": "Recent Failures (Last 1h)",
        "visualization": {
          "type": "table",
          "query": "event_type:extraction AND status:failure AND @timestamp:>now-1h",
          "sortBy": "@timestamp desc",
          "columns": ["file_name", "error_message", "@timestamp", "request_id"]
        }
      }
    ]
  }
}
```

#### 4.2 Create `dashboards/security_events.json`

Kibana dashboard for security monitoring:

```json
{
  "dashboard": {
    "title": "Security Events",
    "panels": [
      {
        "title": "Failed Authentication Attempts (24h)",
        "visualization": {
          "type": "metric",
          "query": "event_type:auth_failure AND @timestamp:>now-24h",
          "metric": "count"
        }
      },
      {
        "title": "Failed Auth by IP",
        "visualization": {
          "type": "table",
          "query": "event_type:auth_failure",
          "metrics": [
            {"field": "client_ip", "type": "terms"},
            {"field": "@timestamp", "type": "count"}
          ]
        }
      },
      {
        "title": "Rate Limit Triggers (24h)",
        "visualization": {
          "type": "metric",
          "query": "event_type:rate_limit AND @timestamp:>now-24h",
          "metric": "count"
        }
      },
      {
        "title": "Rate Limits by Endpoint",
        "visualization": {
          "type": "bar",
          "query": "event_type:rate_limit",
          "xAxis": "endpoint",
          "yAxis": "count"
        }
      }
    ]
  }
}
```

#### 4.3 Create `dashboards/performance.json`

Kibana dashboard for performance monitoring:

```json
{
  "dashboard": {
    "title": "Performance Anomalies",
    "panels": [
      {
        "title": "Average Extraction Time",
        "visualization": {
          "type": "line",
          "query": "event_type:extraction AND status:success",
          "xAxisField": "@timestamp",
          "yAxisField": "avg(extraction_time_seconds)"
        }
      },
      {
        "title": "P95 Extraction Time",
        "visualization": {
          "type": "gauge",
          "query": "event_type:extraction AND status:success",
          "metric": "percentiles(extraction_time_seconds, 95)"
        }
      },
      {
        "title": "Performance Anomalies",
        "visualization": {
          "type": "table",
          "query": "event_type:performance_anomaly",
          "columns": ["metric", "value", "threshold", "@timestamp"]
        }
      }
    ]
  }
}
```

---

### Phase 5: Configuration & Deployment

#### 5.1 Update `requirements.txt`

Add logging dependencies:

```
python-json-logger>=2.0.7
```

#### 5.2 Update `.env.production`

Add ELK configuration variables:

```bash
# ELK Stack Configuration
LOGSTASH_HOST=logstash
LOGSTASH_PORT=5000
ELASTICSEARCH_HOST=elasticsearch
ELASTICSEARCH_PORT=9200
KIBANA_URL=http://kibana:5601

# Log Retention Policy (days)
LOG_RETENTION_SECURITY=90
LOG_RETENTION_OPERATIONAL=30
```

---

## Checklist

### Infrastructure Setup
- [ ] Update `docker-compose.yml` with Elasticsearch, Logstash, Kibana services
- [ ] Create `logstash.conf` with input/filter/output configuration
- [ ] Create `logstash_patterns/` directory
- [ ] Create `scripts/setup_elasticsearch.sh` for ILM policy setup

### Application Logging
- [ ] Create/update `app/core/logging.py` with StructuredLogger
- [ ] Update `app/main.py` to initialize structured logging
- [ ] Update conversion endpoint to log extraction events
- [ ] Add structured logging for security events (auth failures, rate limits)
- [ ] Add structured logging for performance anomalies

### Retention & Policies
- [ ] Create `logstash_retention.conf` with ILM policies
- [ ] Execute `setup_elasticsearch.sh` in Docker startup

### Kibana Dashboards
- [ ] Create `dashboards/extraction_failures.json`
- [ ] Create `dashboards/security_events.json`
- [ ] Create `dashboards/performance.json`
- [ ] Verify dashboards display correct data in Kibana UI

### Tests & Verification
- [ ] Create `tests/test_logging.py` with log ingestion tests
- [ ] Create `tests/test_elk_integration.py` with Elasticsearch integration tests
- [ ] Verify logs appear in Elasticsearch indices
- [ ] Verify retention policies are enforced
- [ ] Verify dashboards populate correctly

### Documentation
- [ ] Create `DOCS/LOGGING.md` with setup and usage guide
- [ ] Update `DOCS/MONITORING.md` to reference log aggregation
- [ ] Document dashboard queries and thresholds

---

## Verification Commands

```bash
# Verify Elasticsearch health
curl -s http://localhost:9200/_cluster/health | jq .

# Check indices
curl -s http://localhost:9200/_cat/indices?v

# View recent logs
curl -s 'http://localhost:9200/docc2context-*/_search' \
  -H 'Content-Type: application/json' \
  -d '{"query": {"match_all": {}}, "size": 10}' | jq .

# Access Kibana
# http://localhost:5601

# Run tests
python -m pytest tests/test_logging.py -v
python -m pytest tests/test_elk_integration.py -v
```

---

## Definition of Done

✅ **Acceptance Criteria (from PRD):**
1. ✅ Logs are aggregated in central location (Elasticsearch)
2. ✅ All security events are logged (auth failures, rate limits, extractions)
3. ✅ Retention policy is enforced (90d security, 30d operational)
4. ✅ Dashboards are functional (Kibana - 3 dashboards)
5. ✅ Can search and filter logs easily (KQL queries in Kibana)

**Implementation validates:**
- Docker services start and health checks pass
- Logs flow from app → Logstash → Elasticsearch
- Kibana connects and displays aggregated logs
- Structured JSON logging includes required fields
- Retention policies apply to indices
- All dashboards populate with sample data
- Tests confirm log ingestion and searchability

---

## Files Modified/Created

**Created:**
- `docker-compose.yml` (updated)
- `logstash.conf`
- `logstash_patterns/` (directory)
- `logstash_retention.conf`
- `scripts/setup_elasticsearch.sh`
- `app/core/logging.py` (new or enhanced)
- `dashboards/extraction_failures.json`
- `dashboards/security_events.json`
- `dashboards/performance.json`
- `tests/test_logging.py`
- `tests/test_elk_integration.py`
- `DOCS/LOGGING.md`

**Modified:**
- `app/main.py`
- `requirements.txt`
- `.env.production`
- `DOCS/MONITORING.md`

---

## Notes & Considerations

1. **Single-node Elasticsearch**: For development/MVP. Production should use multi-node with replication.
2. **Security**: `xpack.security.enabled=false` is for dev. Production must enable X-Pack security.
3. **Storage**: Elasticsearch indices grow with log volume. Monitor disk space and retention policies.
4. **Performance**: Logstash may need tuning if log volume is high (e.g., increase JVM heap).
5. **Backup**: Implement Elasticsearch snapshot/restore for backup strategy.
6. **Disaster Recovery**: Use ILM policies to archive logs to cold storage or S3.

---

**Ready for EXECUTE phase.**
