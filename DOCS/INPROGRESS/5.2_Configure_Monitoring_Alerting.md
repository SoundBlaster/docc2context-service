# Task 5.2 Implementation PRD — Configure Monitoring & Alerting

**Status:** READY FOR EXECUTION
**Priority:** Critical (Blocking Production)
**Effort:** 4-6 hours
**Phase:** 5 (Production Security)

---

## Overview

Set up metrics collection, alerting infrastructure, and monitoring dashboards for production deployment. Configure alerts for:
- High error rates (>10% 5xx errors)
- Extraction failures (suspicious patterns)
- Rate limit triggers
- Resource exhaustion (CPU, memory, disk)

Create alerting playbooks documenting alert meanings and response procedures.

---

## Current State Analysis

### Logging
- ✅ Structured JSON logging configured (`app/core/logging.py`)
- ✅ Request ID tracking implemented
- ✅ Log level configurable via environment
- ❌ No centralized log aggregation yet
- ❌ No metrics collection

### Health Checks
- ✅ `/health` endpoint exists
- ❌ No detailed system metrics
- ❌ No integration with monitoring platform

### Monitoring Infrastructure
- ❌ No Prometheus/DataDog/CloudWatch integration
- ❌ No alert rules configured
- ❌ No dashboards
- ❌ No playbooks

---

## Implementation Approach

**Choice: Prometheus + Alertmanager** (self-hosted, open-source)
- Alternative: DataDog (SaaS, easier but paid)
- Alternative: CloudWatch (AWS-only)

**Recommendation:** Implement both Prometheus (for self-hosted) and documentation for DataDog/CloudWatch configuration.

---

## Implementation Checklist

### Subtask 1: Instrument FastAPI with Prometheus Metrics
**Files:**
- `app/core/metrics.py` (NEW)
- `app/main.py` (modify)
- `requirements.txt` (add `prometheus-client`)

**Implementation:**

1. **Create metrics module:**
```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time

# Define metrics
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint'],
    buckets=(0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10)
)

zip_extractions_total = Counter(
    'zip_extractions_total',
    'Total ZIP extractions',
    ['status']  # success, failed_validation, failed_extraction
)

zip_extraction_duration = Histogram(
    'zip_extraction_duration_seconds',
    'ZIP extraction duration',
    buckets=(0.1, 0.5, 1, 2, 5, 10, 30)
)

extraction_file_count = Histogram(
    'extraction_file_count',
    'Files extracted per ZIP',
    buckets=(1, 10, 100, 1000, 10000)
)

extraction_size_bytes = Histogram(
    'extraction_size_bytes',
    'Size of extracted content',
    buckets=(1024, 10485760, 104857600, 1073741824)  # 1KB, 10MB, 100MB, 1GB
)

active_conversions = Gauge(
    'active_conversions',
    'Number of active conversions'
)

conversion_errors_total = Counter(
    'conversion_errors_total',
    'Total conversion errors',
    ['error_type']  # validation_error, extraction_error, cli_error, timeout
)

resource_usage = Gauge(
    'resource_usage',
    'Current resource usage',
    ['resource_type']  # cpu_percent, memory_mb, disk_free_gb
)
```

2. **Add Prometheus middleware to FastAPI:**
```python
# In app/main.py
from prometheus_client import make_wsgi_app
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from fastapi import Response
from app.core.metrics import (
    request_count, request_duration, active_conversions,
    conversion_errors_total
)

# Add /metrics endpoint
@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

# Add metrics middleware
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    """Middleware to track request metrics"""
    method = request.method
    path = request.url.path

    start = time.time()
    response = await call_next(request)
    duration = time.time() - start

    request_count.labels(
        method=method,
        endpoint=path,
        status=response.status_code
    ).inc()

    request_duration.labels(
        method=method,
        endpoint=path
    ).observe(duration)

    return response
```

3. **Add metrics tracking to conversion endpoints:**
   - Increment `zip_extractions_total` (success/failed)
   - Observe `zip_extraction_duration`
   - Observe `extraction_file_count`
   - Observe `extraction_size_bytes`
   - Increment `conversion_errors_total` on errors

4. **Add resource monitoring:**
```python
# In a background task
import psutil
from app.core.metrics import resource_usage

async def monitor_resources():
    """Background task to monitor resource usage"""
    while True:
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_mb = psutil.virtual_memory().used / 1024 / 1024
            disk_gb = psutil.disk_usage('/').free / 1024 / 1024 / 1024

            resource_usage.labels(resource_type='cpu_percent').set(cpu_percent)
            resource_usage.labels(resource_type='memory_mb').set(memory_mb)
            resource_usage.labels(resource_type='disk_free_gb').set(disk_gb)
        except Exception as e:
            logger.error(f"Error monitoring resources: {e}")

        await asyncio.sleep(30)  # Check every 30 seconds

# Schedule in startup event
```

**Verification:**
```bash
# Start app
python -m uvicorn app.main:app

# Test metrics endpoint
curl http://localhost:8000/metrics | head -50
# Should return Prometheus format metrics
```

**Acceptance Criteria:**
- [ ] `/metrics` endpoint returns Prometheus metrics
- [ ] HTTP request metrics are tracked
- [ ] ZIP extraction metrics are tracked
- [ ] Resource usage metrics are collected
- [ ] Metrics include proper labels for filtering

---

### Subtask 2: Create Prometheus Configuration
**File:** `prometheus.yml` (NEW)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'docc2context-service'

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - localhost:9093

scrape_configs:
  - job_name: 'docc2context'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s
```

**Verification:**
```bash
# Validate config syntax
prometheus --config.file=prometheus.yml --web.enable-lifecycle --dry-run
```

**Acceptance Criteria:**
- [ ] prometheus.yml is valid
- [ ] Can scrape metrics from `/metrics` endpoint
- [ ] Scrape interval is 5-15 seconds

---

### Subtask 3: Create Alert Rules
**File:** `alert_rules.yml` (NEW)

```yaml
groups:
  - name: docc2context_alerts
    interval: 30s
    rules:
      # High error rate alert
      - alert: HighErrorRate
        expr: |
          (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 10% ({{ $value | humanizePercentage }})"
          runbook_url: "file:///docs/playbooks/high_error_rate.md"

      # Extraction failures alert
      - alert: HighExtractionFailureRate
        expr: |
          (sum(rate(zip_extractions_total{status="failed"}[5m])) / sum(rate(zip_extractions_total[5m]))) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High ZIP extraction failure rate"
          description: "Extraction failure rate is {{ $value | humanizePercentage }}"
          runbook_url: "file:///docs/playbooks/extraction_failures.md"

      # Memory exhaustion alert
      - alert: HighMemoryUsage
        expr: resource_usage{resource_type="memory_mb"} > 1800  # 90% of 2GB
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Memory usage is high"
          description: "Memory usage: {{ $value | humanize }}MB"
          runbook_url: "file:///docs/playbooks/memory_exhaustion.md"

      # CPU exhaustion alert
      - alert: HighCPUUsage
        expr: resource_usage{resource_type="cpu_percent"} > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPU usage is high"
          description: "CPU usage: {{ $value | humanize }}%"
          runbook_url: "file:///docs/playbooks/cpu_exhaustion.md"

      # Disk space alert
      - alert: LowDiskSpace
        expr: resource_usage{resource_type="disk_free_gb"} < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Free disk space: {{ $value | humanize }}GB"
          runbook_url: "file:///docs/playbooks/low_disk_space.md"

      # Service down alert
      - alert: ServiceDown
        expr: up{job="docc2context"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "DocC2Context service is down"
          description: "Service has been down for more than 1 minute"
          runbook_url: "file:///docs/playbooks/service_down.md"
```

**Verification:**
```bash
# Validate alert rules
promtool check rules alert_rules.yml
```

**Acceptance Criteria:**
- [ ] alert_rules.yml is valid
- [ ] At least 6 alerts configured
- [ ] Alerts have proper severity labels
- [ ] All alerts link to playbooks

---

### Subtask 4: Create Alertmanager Configuration
**File:** `alertmanager.yml` (NEW)

```yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h

  routes:
    # Critical alerts go to Slack + PagerDuty
    - match:
        severity: critical
      receiver: 'critical'
      group_wait: 10s
      repeat_interval: 1h

    # Warnings go to Slack only
    - match:
        severity: warning
      receiver: 'warning'
      repeat_interval: 4h

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'DocC2Context Alert'
        text: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'

  - name: 'critical'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
    # Could add PagerDuty integration here
    # pagerduty_configs:
    #   - routing_key: 'YOUR_ROUTING_KEY'

  - name: 'warning'
    slack_configs:
      - channel: '#alerts'
        title: 'WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
```

**Verification:**
```bash
# Validate config syntax
amtool config routes
```

**Acceptance Criteria:**
- [ ] alertmanager.yml is valid
- [ ] Slack webhook configured
- [ ] Routing rules separate critical/warning alerts
- [ ] Test alert can be sent and received

---

### Subtask 5: Docker Compose Updates
**File:** `docker-compose.yml` (modify)

Add Prometheus and Alertmanager services:

```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    networks:
      - docc2context-network

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - docc2context-network

volumes:
  prometheus_data:
  alertmanager_data:
```

**Verification:**
```bash
# Start services
docker-compose up prometheus alertmanager

# Test Prometheus UI
curl http://localhost:9090

# Test Alertmanager UI
curl http://localhost:9093
```

**Acceptance Criteria:**
- [ ] Prometheus service starts and scrapes metrics
- [ ] Alertmanager service starts
- [ ] Prometheus UI accessible at http://localhost:9090
- [ ] Alertmanager UI accessible at http://localhost:9093

---

### Subtask 6: Create Alerting Playbooks
**Files:** `DOCS/PLAYBOOKS/` (NEW directory)

Create markdown playbooks for each alert:

#### `high_error_rate.md`
```markdown
# Alert: High Error Rate

## Definition
HTTP error rate (5xx responses) exceeds 10% over 5 minutes.

## Typical Causes
1. Service crash/panic
2. Downstream service unavailable
3. Configuration error
4. Memory/resource exhaustion

## Response Steps
1. Check Prometheus dashboard for patterns
2. Check logs: `docker logs docc2context`
3. Check resource usage (CPU, memory, disk)
4. Check downstream service status
5. If crashed: restart service `docker-compose restart docc2context`
6. If config error: fix and redeploy
7. If resource: scale up container limits

## Escalation
- If unresolved after 15 min: page on-call engineer
- If causing user impact: declare incident
```

#### `extraction_failures.md`
```markdown
# Alert: High Extraction Failure Rate

## Definition
ZIP extraction failures exceed 20% over 5 minutes.

## Typical Causes
1. Malformed ZIP files from users
2. Resource limits being hit
3. ZIP bomb protection triggered
4. Disk space exhausted

## Response Steps
1. Check logs for error pattern: `docker logs -f docc2context | grep extraction`
2. Check which error type is most common
3. If disk full: clean /tmp or scale storage
4. If resource limit: increase container limits
5. If ZIP bombs: confirm limits are working (check SECURITY_CHECKLIST)

## Escalation
- If legitimate files being rejected: review validation rules
- If persistent: escalate to security team
```

#### `memory_exhaustion.md`
```markdown
# Alert: High Memory Usage

## Definition
Memory usage exceeds 1800MB (90% of 2GB limit).

## Typical Causes
1. Large ZIP extraction (legitimate)
2. Memory leak in code
3. Concurrent conversions
4. Insufficient container limits

## Response Steps
1. Check current memory: `docker stats docc2context`
2. Check for long-running conversions
3. Check if this is during high-traffic period
4. If OOM Kill expected: increase limit in docker-compose.yml
5. If leak suspected: check logs for patterns

## Escalation
- If consistently near limit: increase container memory
- If crashes with OOM: immediate production impact - scale horizontally
```

#### `cpu_exhaustion.md`
```markdown
# Alert: High CPU Usage

## Definition
CPU usage exceeds 80% for 5+ minutes.

## Typical Causes
1. Concurrent conversions (legitimate)
2. CLI tool (swift) CPU-intensive
3. Inefficient code

## Response Steps
1. Check current CPU: `docker stats docc2context`
2. Check process-level CPU: `docker top docc2context`
3. Verify swift CLI is not stuck
4. Check for queue buildup
5. If temporary: no action needed
6. If sustained: scale horizontally (add more containers)

## Escalation
- If causing slowness: scale to additional instances
```

#### `low_disk_space.md`
```markdown
# Alert: Low Disk Space

## Definition
Free disk space drops below 1GB.

## Typical Causes
1. /tmp filling up with workspace leftovers
2. Logs not rotating
3. Container storage driver issue

## Response Steps
1. Check disk usage: `df -h /tmp`
2. Check workspace cleanup: `ls -la /tmp/swift-conv-* | wc -l`
3. Trigger manual cleanup: restart cleanup job
4. Check logs location and rotation
5. If issue persists: scale persistent volume

## Escalation
- If workspace cleanup failing: page on-call
- If critical: immediately stop accepting uploads
```

#### `service_down.md`
```markdown
# Alert: Service Down

## Definition
Service is not responding to health checks.

## Response Steps
1. Check service status: `docker-compose ps docc2context`
2. Check logs: `docker logs docc2context`
3. Restart service: `docker-compose restart docc2context`
4. Check dependent services (swift, etc.)
5. Check network connectivity

## Escalation
- IMMEDIATE: This is production outage
- Page on-call engineer immediately
- Post to #incidents channel
- If restart doesn't work: escalate to infrastructure team
```

**Verification:**
- [ ] All 6+ playbooks created
- [ ] Each playbook has response steps
- [ ] Playbooks link back to alerts

---

### Subtask 7: Create Monitoring Dashboard
**Files:** `DOCS/MONITORING.md` (NEW)

Document how to access and use monitoring:

```markdown
# Monitoring Dashboard

## Prometheus
- **URL:** http://localhost:9090
- **Purpose:** Metrics collection and graphing
- **Common Queries:**
  - `http_requests_total{status="500"}` - 500 errors
  - `rate(http_requests_total[5m])` - request rate
  - `zip_extraction_duration_seconds_bucket` - extraction times

## Alertmanager
- **URL:** http://localhost:9093
- **Purpose:** Alert routing and management
- **Key Actions:**
  - View active alerts
  - View alert history
  - Test alert routing

## Testing Alerts

### Test high error rate alert:
1. Simulate errors: `curl http://localhost:8000/api/v1/convert` 100 times
2. Check Prometheus: `sum(rate(http_requests_total{status=~"5.."}[5m]))`
3. Wait 2+ minutes for alert to fire
4. Check Alertmanager for notification

### Test memory alert:
1. Start conversion with huge ZIP
2. Monitor: `docker stats docc2context`
3. Alert should fire when >1800MB

## Integration Examples

### Slack Integration
1. Create Slack webhook: https://api.slack.com/messaging/webhooks
2. Update `alertmanager.yml`: `slack_api_url: 'YOUR_WEBHOOK'`
3. Restart alertmanager: `docker-compose restart alertmanager`
4. Test: `amtool alert add TEST severity=warning`

### PagerDuty Integration
1. Get routing key from PagerDuty
2. Add to `alertmanager.yml` pagerduty_configs
3. Test alert delivery

### CloudWatch Integration (AWS)
1. Update FastAPI to export CloudWatch metrics
2. Install CloudWatch exporter
3. Configure IAM role with CloudWatch permissions
```

**Verification:**
- [ ] DOCS/MONITORING.md created
- [ ] Includes Prometheus and Alertmanager URLs
- [ ] Includes testing procedures
- [ ] Includes integration examples

---

### Subtask 8: Add Metrics to Requirements
**File:** `requirements.txt`

Add:
```
prometheus-client>=0.16.0
psutil>=5.9.0
```

**Verification:**
```bash
pip install prometheus-client psutil
python -c "import prometheus_client; print(prometheus_client.__version__)"
```

---

## Testing & Verification

### Integration Tests
**File:** `tests/test_monitoring.py` (NEW)

```python
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_metrics_endpoint_exists():
    """Test that /metrics endpoint exists"""
    response = client.get("/metrics")
    assert response.status_code == 200
    assert b"http_requests_total" in response.content

def test_metrics_format():
    """Test that metrics are in Prometheus format"""
    response = client.get("/metrics")
    content = response.text
    # Prometheus format has HELP, TYPE, and metric lines
    assert "# HELP" in content
    assert "# TYPE" in content

def test_request_metrics_recorded():
    """Test that request metrics are recorded"""
    # Make a request
    client.get("/health")

    # Get metrics
    response = client.get("/metrics")
    assert b"http_requests_total" in response.content
    assert b"/health" in response.content
```

**Verification:**
```bash
pytest tests/test_monitoring.py -v
```

---

## Definition of Done

✅ All acceptance criteria met:
- [ ] Prometheus metrics instrumentation complete
- [ ] /metrics endpoint returns valid Prometheus metrics
- [ ] Prometheus configuration created and validated
- [ ] Alert rules defined (6+ alerts)
- [ ] Alertmanager configuration created
- [ ] Docker Compose includes Prometheus and Alertmanager
- [ ] All alerting playbooks created
- [ ] Monitoring dashboard documentation complete
- [ ] Integration tests passing
- [ ] All 3 metrics types tracked (HTTP, ZIP extraction, resources)

---

## Files to Create/Modify

**New Files:**
1. `app/core/metrics.py` - Prometheus metrics definitions
2. `prometheus.yml` - Prometheus configuration
3. `alert_rules.yml` - Alert rule definitions
4. `alertmanager.yml` - Alertmanager configuration
5. `DOCS/PLAYBOOKS/` - Alerting playbooks (6+ files)
6. `DOCS/MONITORING.md` - Monitoring documentation
7. `tests/test_monitoring.py` - Monitoring tests

**Modified Files:**
1. `app/main.py` - Add /metrics endpoint and middleware
2. `docker-compose.yml` - Add Prometheus and Alertmanager services
3. `requirements.txt` - Add prometheus-client and psutil
4. `app/core/logging.py` - Add resource monitoring if needed

---

## Success Criteria

After execution:
```bash
# Metrics endpoint works
curl http://localhost:8000/metrics | grep http_requests_total

# Prometheus scrapes metrics
curl http://localhost:9090/api/v1/targets

# Alertmanager receives alerts
curl http://localhost:9093/api/v1/alerts

# All tests pass
pytest tests/test_monitoring.py -v
```

---

## Risk Mitigation

- **Risk:** Prometheus storage fills disk
  - **Mitigation:** Set retention in prometheus.yml (30 days default)

- **Risk:** Alert fatigue from too many alerts
  - **Mitigation:** Tune thresholds based on baseline metrics

- **Risk:** Performance impact from metrics collection
  - **Mitigation:** Metrics overhead is <1% CPU typically

---

## Next Steps After EXECUTE

1. Commit changes with: "Task 5.2: Configure Monitoring & Alerting"
2. Verify all tests pass
3. Run Prometheus and Alertmanager locally
4. Test alert firing with manual trigger
5. Proceed to ARCHIVE
6. Select Task 5.3 (Log Aggregation)
